{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heroes 3 Planner Experiments\n",
    "\n",
    "This notebook will be used for various experiments, data analysis, and visualization related to the HOMM3 planner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Import necessary libraries\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Placeholder for future experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Baseline Greedy Algorithm Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components from the heroes3_planner package\n",
    "# Assuming the notebook is run from the 'algorithms-playground' root or the package is installed\n",
    "import sys\n",
    "sys.path.append('../') # Adjust if necessary to find the package\n",
    "\n",
    "from heroes3_planner.simulator import Hero, GameSimulator\n",
    "from heroes3_planner.planners.greedy import best_move_greedy, get_node_reward # For inspection if needed\n",
    "from heroes3_planner.planners.astar import a_star_planner # Import A* planner\n",
    "from heroes3_planner.scripts.graph_builder import TERRAIN_MOVEMENT_COSTS, DIAGONAL_FACTOR # For reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup a Sample Game State (Graph, Hero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_experiment_graph():\n",
    "    graph = nx.Graph()\n",
    "    hero_start_pos = (0,0)\n",
    "\n",
    "    nodes_data = {\n",
    "        hero_start_pos: {\"terrain_type\": \"grass\", \"objects\": [], \"reward\": 0},\n",
    "        (0,1): {\"terrain_type\": \"dirt\", \"objects\": [{\"type\": \"resource_pile\", \"resource\": \"gold\", \"amount\": 100}], \"reward\": 0},\n",
    "        (1,0): {\"terrain_type\": \"swamp\", \"objects\": [], \"reward\": 10},\n",
    "        (1,1): {\"terrain_type\": \"grass\", \"objects\": [{\"type\": \"monster\", \"name\": \"Goblins\", \"strength\": 30}], \"reward\": 200, \"guard_strength\": 30},\n",
    "        (2,0): {\"terrain_type\": \"sand\", \"objects\": [{\"type\": \"artifact\", \"id\": \"sword\", \"value\": 150}], \"reward\": 0},\n",
    "        (2,1): {\"terrain_type\": \"rough\", \"objects\": [{\"type\": \"mine\", \"mine_type\": \"ore_mine\", \"flag_reward\": 250}], \"reward\": 0},\n",
    "        (0,2): {\"terrain_type\": \"grass\", \"objects\": [{\"type\": \"resource_pile\", \"resource\": \"wood\", \"amount\": 5}], \"reward\": 0},\n",
    "        (1,2): {\"terrain_type\": \"grass\", \"objects\": [], \"reward\": 5},\n",
    "        (2,2): {\"terrain_type\": \"grass\", \"objects\": [{\"type\": \"resource_pile\", \"resource\": \"gold\", \"amount\": 300}], \"reward\": 0}\n",
    "    }\n",
    "    for node, data in nodes_data.items():\n",
    "        graph.add_node(node, **data)\n",
    "\n",
    "    edges_data = [\n",
    "        (hero_start_pos, (0,1), 100),\n",
    "        (hero_start_pos, (1,0), 120),\n",
    "        ((0,1), (1,1), 80),\n",
    "        ((0,1), (0,2), 100),\n",
    "        ((1,0), (1,1), 100),\n",
    "        ((1,0), (2,0), 150),\n",
    "        ((1,1), (1,2), 100),\n",
    "        ((1,1), (2,1), 90),\n",
    "        ((2,0), (2,1), 130),\n",
    "        ((1,2), (2,2), 100),\n",
    "        ((2,1), (2,2), 100)\n",
    "    ]\n",
    "    for u, v, w in edges_data:\n",
    "        graph.add_edge(u,v, weight=w)\n",
    "    return graph\n",
    "\n",
    "experiment_graph = create_sample_experiment_graph()\n",
    "hero_for_experiment = Hero(hero_id=\"exp_hero\", pos=(0,0), base_movement_points=300, army={\"pikemen\": 25}) # Str 250\n",
    "combat_rho_experiment = 1.5\n",
    "\n",
    "# Initialize Simulator\n",
    "sim = GameSimulator(graph=experiment_graph, heroes=[hero_for_experiment], combat_rho=combat_rho_experiment)\n",
    "\n",
    "print(f\"Initial Hero State: {sim.get_hero()}\")\n",
    "print(f\"Initial Gold: {sim.get_hero().inventory['resources'].get('gold',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Greedy Planner for a Few Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy_simulation(simulator: GameSimulator, hero_id: str, num_days: int):\n",
    "    total_reward_collected = {} # {'gold': X, 'wood': Y}\n",
    "    path_taken_daily = []\n",
    "\n",
    "    for day in range(1, num_days + 1):\n",
    "        current_hero = simulator.get_hero(hero_id)\n",
    "        if not current_hero:\n",
    "            print(f\"Day {day}: Hero {hero_id} not found.\")\n",
    "            break\n",
    "        \n",
    "        simulator.log_action(f\"--- Starting Day {day} for Hero {hero_id} ---\")\n",
    "        daily_path = [current_hero.pos]\n",
    "        \n",
    "        while current_hero.current_movement_points > 0:\n",
    "            best_path_segment = best_move_greedy(current_hero, simulator.graph, simulator.combat_rho)\n",
    "            \n",
    "            if not best_path_segment or len(best_path_segment) < 2: # No move or path is just current pos\n",
    "                simulator.log_action(f\"No beneficial greedy move found or no path. Ending turn for day {day}.\")\n",
    "                break\n",
    "            \n",
    "            # The greedy planner returns the full path from current_pos.\n",
    "            # The simulator's step function expects a single target_pos for a move.\n",
    "            # We'll simulate moving one step at a time along this path.\n",
    "            \n",
    "            next_tile_in_path = best_path_segment[1] # The first step in the recommended path\n",
    "            \n",
    "            action_result = simulator.step(hero_id, \"move\", target_pos=next_tile_in_path)\n",
    "            daily_path.append(current_hero.pos)\n",
    "            \n",
    "            if action_result.get(\"status\") != \"success\":\n",
    "                simulator.log_action(f\"Move to {next_tile_in_path} failed or resulted in end of movement: {action_result.get('message')}\")\n",
    "                break # End current hero's turn for the day if move fails or combat ends turn\n",
    "            \n",
    "            # Update rewards (simplified: assume interaction result contains all rewards from that step)\n",
    "            interaction_rewards = action_result.get(\"interaction\", {}).get(\"rewards\", {})\n",
    "            for res_type, amount in interaction_rewards.get(\"resources\", {}).items():\n",
    "                total_reward_collected[res_type] = total_reward_collected.get(res_type, 0) + amount\n",
    "            # Could also track artifacts, XP etc.\n",
    "\n",
    "        path_taken_daily.append(daily_path)\n",
    "        simulator.end_day() # Resets hero movement for next day\n",
    "        \n",
    "    simulator.log_action(f\"--- Greedy Simulation Ended after {num_days} days ---\")\n",
    "    return total_reward_collected, path_taken_daily\n",
    "\n",
    "# Run greedy simulation\n",
    "num_simulation_days = 3\n",
    "greedy_rewards, greedy_paths = run_greedy_simulation(sim, hero_for_experiment.id, num_simulation_days)\n",
    "\n",
    "print(f\"\\nGreedy Simulation Results after {num_simulation_days} days:\")\n",
    "print(f\"Total rewards collected: {greedy_rewards}\")\n",
    "print(f\"Paths taken: {greedy_paths}\")\n",
    "print(f\"Final Hero State: {sim.get_hero()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare with a Random Move Strategy (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_simulation(simulator: GameSimulator, hero_id: str, num_days: int):\n",
    "    total_reward_collected = {}\n",
    "    path_taken_daily = []\n",
    "    \n",
    "    # Reset simulator state for a fair comparison (e.g. graph objects)\n",
    "    # This requires a deep copy of the initial graph or re-initialization\n",
    "    # For simplicity, we'll reuse the graph but note this limitation.\n",
    "    \n",
    "    for day in range(1, num_days + 1):\n",
    "        current_hero = simulator.get_hero(hero_id)\n",
    "        if not current_hero:\n",
    "            break\n",
    "        \n",
    "        simulator.log_action(f\"--- Starting RANDOM Day {day} for Hero {hero_id} ---\")\n",
    "        daily_path = [current_hero.pos]\n",
    "        \n",
    "        while current_hero.current_movement_points > 0:\n",
    "            neighbors = list(simulator.graph.neighbors(current_hero.pos))\n",
    "            if not neighbors:\n",
    "                simulator.log_action(f\"No neighbors to move to. Ending turn for day {day}.\")\n",
    "                break\n",
    "            \n",
    "            random_target_tile = random.choice(neighbors)\n",
    "            \n",
    "            action_result = simulator.step(hero_id, \"move\", target_pos=random_target_tile)\n",
    "            daily_path.append(current_hero.pos)\n",
    "\n",
    "            if action_result.get(\"status\") != \"success\":\n",
    "                simulator.log_action(f\"Random move to {random_target_tile} failed or ended movement: {action_result.get('message')}\")\n",
    "                break\n",
    "            \n",
    "            interaction_rewards = action_result.get(\"interaction\", {}).get(\"rewards\", {})\n",
    "            for res_type, amount in interaction_rewards.get(\"resources\", {}).items():\n",
    "                total_reward_collected[res_type] = total_reward_collected.get(res_type, 0) + amount\n",
    "        \n",
    "        path_taken_daily.append(daily_path)\n",
    "        simulator.end_day()\n",
    "        \n",
    "    simulator.log_action(f\"--- Random Simulation Ended after {num_days} days ---\")\n",
    "    return total_reward_collected, path_taken_daily\n",
    "\n",
    "# Re-initialize simulator and hero for random strategy for a cleaner comparison\n",
    "experiment_graph_for_random = create_sample_experiment_graph() # Fresh graph\n",
    "hero_for_random_experiment = Hero(hero_id=\"exp_hero_random\", pos=(0,0), base_movement_points=300, army={\"pikemen\": 25})\n",
    "sim_random = GameSimulator(graph=experiment_graph_for_random, heroes=[hero_for_random_experiment], combat_rho=combat_rho_experiment)\n",
    "\n",
    "print(f\"\\nInitial Hero State (Random Sim): {sim_random.get_hero()}\")\n",
    "\n",
    "random_rewards, random_paths = run_random_simulation(sim_random, hero_for_random_experiment.id, num_simulation_days)\n",
    "\n",
    "print(f\"\\nRandom Simulation Results after {num_simulation_days} days:\")\n",
    "print(f\"Total rewards collected: {random_rewards}\")\n",
    "print(f\"Paths taken: {random_paths}\")\n",
    "print(f\"Final Hero State (Random Sim): {sim_random.get_hero()}\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Greedy Rewards: {greedy_rewards}\")\n",
    "print(f\"Random Rewards: {random_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The above simulation is simplified. A full comparison would involve:\n",
    "- Resetting the game state (especially collected objects on the graph) perfectly between simulations.\n",
    "- Running multiple iterations of the random strategy to get an average performance.\n",
    "- More sophisticated reward tracking (e.g., value of artifacts, XP, captured mines/dwellings beyond simple resource counts).\n",
    "- The greedy planner currently only plans one step ahead based on immediate reward/cost. A multi-day simulation reveals how these short-sighted decisions play out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: A* Planner Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run A* Planner for a Few Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Re-initialize simulator and hero for A* strategy\n",
    "experiment_graph_astar = create_sample_experiment_graph() # Fresh graph\n",
    "hero_for_astar_experiment = Hero(hero_id=\"exp_hero_astar\", pos=(0,0), base_movement_points=300, army={\"pikemen\": 25})\n",
    "sim_astar = GameSimulator(graph=experiment_graph_astar, heroes=[hero_for_astar_experiment], combat_rho=combat_rho_experiment)\n",
    "\n",
    "print(f\"\\nInitial Hero State (A* Sim): {sim_astar.get_hero()}\")\n",
    "\n",
    "num_astar_plan_days = 2 # Let's plan for 2 days with A*\n",
    "\n",
    "start_time_astar = time.time()\n",
    "astar_planned_path = a_star_planner(\n",
    "    initial_hero=sim_astar.get_hero(),\n",
    "    graph=sim_astar.graph,\n",
    "    combat_rho=sim_astar.combat_rho,\n",
    "    max_days=num_astar_plan_days,\n",
    "    base_daily_movement=sim_astar.get_hero().base_movement_points\n",
    ")\n",
    "end_time_astar = time.time()\n",
    "astar_computation_time = end_time_astar - start_time_astar\n",
    "\n",
    "print(f\"A* Planned Path (for {num_astar_plan_days} days): {astar_planned_path}\")\n",
    "print(f\"A* Computation Time: {astar_computation_time:.4f} seconds\")\n",
    "\n",
    "# Simulate the execution of the A* path\n",
    "astar_collected_rewards = {}\n",
    "if astar_planned_path and len(astar_planned_path) > 1:\n",
    "    # The simulator's step function handles one move at a time.\n",
    "    # We need to feed it the A* path tile by tile.\n",
    "    # The A* path includes the start node, so skip it for moves.\n",
    "    current_day_in_astar_sim = 1\n",
    "    sim_astar.get_hero().reset_daily_movement() # Ensure hero starts fresh for path execution\n",
    "\n",
    "    for i in range(1, len(astar_planned_path)):\n",
    "        target_tile = astar_planned_path[i]\n",
    "        hero_obj = sim_astar.get_hero()\n",
    "        \n",
    "        # Check if new day starts (simplified check based on movement points or explicit day end in path)\n",
    "        # A more robust way would be for A* to return actions including 'end_day'.\n",
    "        # For now, assume path implies continuous movement until MP runs out, then day ends.\n",
    "        move_cost = sim_astar.graph.edges[hero_obj.pos, target_tile].get('weight', 100)\n",
    "        if hero_obj.current_movement_points < move_cost:\n",
    "            sim_astar.log_action(f\"A* EXEC: Ran out of MP for day {current_day_in_astar_sim}, trying to move from {hero_obj.pos} to {target_tile}.\")\n",
    "            sim_astar.end_day() # This also resets active hero's MP\n",
    "            current_day_in_astar_sim +=1\n",
    "            if current_day_in_astar_sim > num_astar_plan_days:\n",
    "                sim_astar.log_action(f\"A* EXEC: Exceeded planned day horizon ({num_astar_plan_days} days). Stopping execution.\")\n",
    "                break\n",
    "        \n",
    "        action_result = sim_astar.step(hero_obj.id, \"move\", target_pos=target_tile)\n",
    "        \n",
    "        if action_result.get(\"status\") != \"success\":\n",
    "            sim_astar.log_action(f\"A* EXEC: Move to {target_tile} failed: {action_result.get('message')}. Stopping execution.\")\n",
    "            break\n",
    "            \n",
    "        interaction_rewards = action_result.get(\"interaction\", {}).get(\"rewards\", {})\n",
    "        for res_type, amount in interaction_rewards.get(\"resources\", {}).items():\n",
    "            astar_collected_rewards[res_type] = astar_collected_rewards.get(res_type, 0) + amount\n",
    "else:\n",
    "    print(\"A* planner did not return a valid path.\")\n",
    "\n",
    "print(f\"\\nA* Simulation Results after executing planned path (up to {num_astar_plan_days} days):\")\n",
    "print(f\"Total rewards collected by A*: {astar_collected_rewards}\")\n",
    "print(f\"Final Hero State (A* Sim): {sim_astar.get_hero()}\")\n",
    "print(f\"Final Day (A* Sim): {sim_astar.current_day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparison with Greedy and Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Overall Comparison ---\")\n",
    "print(f\"Greedy Rewards (simulated for {num_simulation_days} days): {greedy_rewards}\")\n",
    "print(f\"Random Rewards (simulated for {num_simulation_days} days): {random_rewards}\")\n",
    "print(f\"A* Rewards (planned for {num_astar_plan_days} days, then executed): {astar_collected_rewards}\")\n",
    "print(f\"A* Computation Time for {num_astar_plan_days}-day plan: {astar_computation_time:.4f} seconds\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "planners = ['Greedy', 'Random', 'A*']\n",
    "# Extract 'gold' for simplicity in plotting, or sum of all resource values\n",
    "rewards_gold = [\n",
    "    greedy_rewards.get('gold', 0),\n",
    "    random_rewards.get('gold', 0),\n",
    "    astar_collected_rewards.get('gold', 0)\n",
    "]\n",
    "\n",
    "plt.bar(planners, rewards_gold, color=['blue', 'orange', 'green'])\n",
    "plt.title('Comparison of Collected Gold by Planners')\n",
    "plt.ylabel('Gold Collected')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote on A* path execution: The simulation loop for A* path execution is basic.\")\n",
    "print(\"It assumes the A* path is a sequence of tiles. A more robust execution would handle day ends based on A*'s plan details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7" # Or your python version
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
